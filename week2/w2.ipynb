{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Model-Free vs. Model-Based Reinforcement Learning\n",
    "\n",
    "In **Model-Based Reinforcement Learning**, the agent relies on a model of the environment to make decisions. This model predicts the next state and the expected reward for actions taken, allowing the agent to plan ahead by simulating future states. This approach requires either prior knowledge or assumptions about the environment's dynamics.\n",
    "\n",
    "Contrastingly, **Model-Free Reinforcement Learning** does not rely on a model of the environment. Instead, the agent learns from the observed outcomes of its actions, without needing to predict future states or rewards. This method is more adaptable to complex or unknown environments since it learns directly from experience.\n",
    "\n",
    "Understanding the distinction between these approaches is crucial for grasping the nuances of various RL algorithms and their applications.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92288c6704ff4357"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model-Based Reinforcement Learning Example\n",
    "\n",
    "A straightforward example of model-based RL is using Dynamic Programming (DP) methods, like Value Iteration, in a known environment. Consider a grid where an agent moves to reach a goal, avoiding obstacles. The environment's dynamics, including transition probabilities $P(s'|s, a)$ and rewards $R(s, a, s')$, are fully known.\n",
    "\n",
    "The Value Iteration algorithm updates the value of each state using the Bellman optimality equation:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s, a) [R(s, a, s') + \\gamma V_k(s')]$$\n",
    "\n",
    "This approach efficiently computes the optimal policy in environments where the dynamics are known and static, showcasing the power of model-based RL in planning and decision-making.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9460a1576cca5d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transition to Model-Free Learning\n",
    "\n",
    "While model-based methods are powerful when a complete model of the environment is available, many real-world problems do not provide such luxury. Model-free reinforcement learning comes into play in these scenarios, learning optimal actions through trial and error without a predefined model of the environment. A prime example of model-free learning is Q-Learning, which we will explore next.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69284ed85e4b3a43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction to Q-Learning\n",
    "\n",
    "Q-Learning is a cornerstone of model-free reinforcement learning that allows agents to learn the quality of actions, denoted as Q-values, in given states without a model of the environment. It aims to find an optimal policy by learning the value of taking an action in a particular state, guiding the agent toward actions that increase cumulative rewards.\n",
    "\n",
    "### Key Concepts of Q-Learning:\n",
    "\n",
    "- **Q-Value or Action-Value**: Represents the expected future rewards for an action taken in a given state.\n",
    "- **Policy**: A strategy that the agent employs to determine the next action based on the current state.\n",
    "- **Reward**: Feedback from the environment in response to an action, indicating the value of the action.\n",
    "- **State**: A representation of the current situation in the environment.\n",
    "- **Action**: A set of all possible moves or decisions the agent can make.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34f39bfaad509a1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding Q-Value\n",
    "\n",
    "In reinforcement learning, the **Q-value** or **action-value** is a measure that represents the total expected rewards an agent can obtain, starting from a given state, taking a particular action, and then following an optimal policy thereafter. Essentially, Q-values guide the agent in choosing the best action in each state by estimating the future rewards for those actions.\n",
    "\n",
    "## Formal Definition\n",
    "\n",
    "The Q-value for a state-action pair $\\((s, a)\\)$ is denoted as $\\(Q(s, a)\\)$ and is defined by the equation:\n",
    "\n",
    "$$\\[\n",
    "Q(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) \\max_{a'} Q(s', a')\n",
    "\\]$$\n",
    "\n",
    "where:\n",
    "- $\\(R(s, a)\\)$ is the immediate reward received after taking action $\\(a\\)$ in state $\\(s\\)$,\n",
    "- $\\(\\gamma\\)$ is the discount factor, which balances the importance of immediate and future rewards,\n",
    "- $\\(P(s'|s, a)\\)$ is the probability of transitioning to state $\\(s'\\)$ after taking action $\\(a\\)$ in state $\\(s\\)$,\n",
    "- $\\(\\max_{a'} Q(s', a')\\)$ represents the maximum expected future rewards from the next state $\\(s'\\)$, across all possible actions $\\(a'\\)$.\n",
    "\n",
    "## Purpose and Use\n",
    "\n",
    "- **Decision Making**: The primary use of Q-values in reinforcement learning is to make informed decisions by selecting actions with the highest expected rewards.\n",
    "- **Policy Derivation**: An optimal policy $\\(\\pi^*\\)$ can be directly derived from Q-values, where for each state $\\(s\\)$, the action $\\(a\\)$ that maximizes $\\(Q(s, a)\\)$ is chosen.\n",
    "\n",
    "## In Q-Learning\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn the Q-values for all state-action pairs directly from experience, without needing a model of the environment. It updates Q-values using the Temporal Difference (TD) learning approach, refining estimates based on the difference between observed rewards and current Q-value estimates.\n",
    "\n",
    "By iteratively updating Q-values towards their true values, Q-Learning helps the agent learn an optimal strategy for navigating the environment to maximize cumulative rewards.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1fc055799679ee1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "Temporal Difference (TD) Learning is a central concept in reinforcement learning, sitting at the intersection of Monte Carlo (MC) methods and Dynamic Programming (DP). Unlike MC methods, which wait until the end of an episode to update value estimates based on the total accumulated reward, TD Learning updates estimates incrementally after each step of the episode. This approach allows TD Learning to learn directly from raw experience without a model of the environment's dynamics, similar to MC, yet it updates estimates based on other learned estimates, without waiting for a final outcome, akin to DP.\n",
    "\n",
    "## Key Principles of TD Learning\n",
    "\n",
    "TD Learning focuses on the idea of learning by the difference, or \"temporal difference\", between estimates at successive time steps. The core of TD Learning is the TD error, a measure of the difference between:\n",
    "\n",
    "1. The predicted value of a state or state-action pair, and\n",
    "2. The observed reward plus the predicted value of the subsequent state or state-action pair.\n",
    "\n",
    "The TD error for state-value prediction is given by:\n",
    "\n",
    "$$TD\\ error = (R_{t+1} + \\gamma V(S_{t+1})) - V(S_t)$$\n",
    "\n",
    "where:\n",
    "- $\\(R_{t+1}\\)$ is the reward received after transitioning from state $\\(S_t\\)$ to state $\\(S_{t+1}\\)$,\n",
    "- $\\(\\gamma\\)$ is the discount factor, determining the present value of future rewards,\n",
    "- $\\(V(S_t)\\)$ is the estimated value of state $\\(S_t\\)$,\n",
    "- $\\(V(S_{t+1})\\)$ is the estimated value of the subsequent state $\\(S_{t+1}\\)$.\n",
    "\n",
    "For action-value prediction (as in Q-Learning), the TD error becomes:\n",
    "\n",
    "$$TD\\ error = (R_{t+1} + \\gamma \\max_{a}Q(S_{t+1}, a)) - Q(S_t, A_t)$$\n",
    "\n",
    "where:\n",
    "- $\\(Q(S_t, A_t)\\)$ is the estimated value of taking action $\\(A_t\\)$ in state $\\(S_t\\)$,\n",
    "- $\\(\\max_{a}Q(S_{t+1}, a)\\)$ represents the estimated maximum future reward achievable from the next state $\\(S_{t+1}\\)$, following the best action $\\(a\\)$.\n",
    "\n",
    "## Advantages of TD Learning\n",
    "\n",
    "- **Efficiency**: TD Learning can update value estimates based on partial sequences, without waiting for the episode to conclude, making it efficient for learning in continuous and non-terminating environments.\n",
    "- **Flexibility**: It can learn from experience generated under any policy, making it applicable to a wide range of problems.\n",
    "- **Online and Incremental**: TD methods are inherently online and incremental, allowing them to learn directly from experience in a step-by-step manner.\n",
    "\n",
    "TD Learning serves as the foundation for several powerful reinforcement learning algorithms, including Q-Learning and SARSA, enabling agents to learn optimal policies in complex environments by experiencing and interacting with the environment.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba782ddcdcfa5afc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Q-Learning Algorithm\n",
    "\n",
    "The Q-Learning algorithm updates the Q-values using the equation:\n",
    "\n",
    "$$Q(s, a) = Q(s, a) + \\alpha [R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "Where:\n",
    "- $\\(s\\)$ is the current state.\n",
    "- $\\(a\\)$ is the current action.\n",
    "- $\\(R(s, a)\\)$ is the reward received after taking action $\\(a\\)$ in state $\\(s\\)$.\n",
    "- $\\(\\gamma\\)$ is the discount factor, determining the importance of future rewards.\n",
    "- $\\(\\alpha\\)$ is the learning rate, determining to what extent the newly acquired information will override the old information.\n",
    "- $\\(\\max_{a'} Q(s', a')\\)$ is the estimate of optimal future value.\n",
    "\n",
    "Through repeated updates, the Q-Learning algorithm converges to the optimal policy that maximizes the cumulative reward.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a4bd42fb8fc2d85"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the size of the state space and action space\n",
    "state_size = 10  # Example: 10 states\n",
    "action_size = 4  # Example: 4 actions\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "Q_table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:08.041215Z",
     "start_time": "2024-02-23T23:32:08.023281Z"
    }
   },
   "id": "89e94588cd956512",
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploration vs. Exploitation\n",
    "\n",
    "In Q-Learning, the agent must balance between exploring uncharted actions (exploration) and leveraging known actions that yield high rewards (exploitation). This balance is crucial for the agent to learn an optimal policy.\n",
    "\n",
    "- **Exploration** allows the agent to discover new strategies that may yield higher rewards.\n",
    "- **Exploitation** uses the agent's current knowledge to choose actions that are known to yield high rewards.\n",
    "\n",
    "A common strategy to balance exploration and exploitation is the ε-greedy strategy, where the agent explores randomly with probability ε and exploits the best-known action with probability 1-ε.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a95710daca67ccb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "epsilon = 1.0  # Starting value of epsilon\n",
    "decay_rate = 0.01  # Decay rate for epsilon\n",
    "min_epsilon = 0.01  # Minimum value of epsilon\n",
    "\n",
    "def choose_action(state, Q_table):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice(action_size)  # Explore action space\n",
    "    else:\n",
    "        action = np.argmax(Q_table[state, :])  # Exploit learned values\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:08.153161Z",
     "start_time": "2024-02-23T23:32:08.143164Z"
    }
   },
   "id": "ca9259b181b6e22f",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_Q_table(state, action, reward, next_state, Q_table, alpha, gamma):\n",
    "    # Q-value update formula\n",
    "    Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q_table[next_state, :]) - Q_table[state, action])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:32:08.200146Z",
     "start_time": "2024-02-23T23:32:08.189151Z"
    }
   },
   "id": "7890bd8b44a6dbb3",
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "795bf854a3fcd4f4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Gridworld environment\n",
    "class Gridworld:\n",
    "    def __init__(self, width=5, height=4):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (height - 1, width - 1)\n",
    "        self.obstacles = [(1, 1), (2, 2), (3, 1)]\n",
    "        self.hazard = [(0, 3)]\n",
    "        self.grid = np.zeros((height, width))\n",
    "        \n",
    "        # Mark obstacles, hazard, start, and goal in the grid\n",
    "        self.grid[self.start] = 2  # 2 for start\n",
    "        self.grid[self.goal] = 3   # 3 for goal\n",
    "        for obs in self.obstacles:\n",
    "            self.grid[obs] = -1  # -1 for obstacles\n",
    "        for haz in self.hazard:\n",
    "            self.grid[haz] = -2  # -2 for hazard\n",
    "\n",
    "        self.state = self.start\n",
    "        self.actions = [0, 1, 2, 3]  # Up, Down, Left, Right\n",
    "    \n",
    "        # Initialize Q-table\n",
    "        self.num_states = self.width * self.height\n",
    "        self.Q_table = np.zeros((self.num_states, len(self.actions)))\n",
    "\n",
    "\n",
    "        self.goal_reward = 10\n",
    "        self.hazard_reward = -10\n",
    "        self.obstacle_reward = -5\n",
    "        self.free_space_reward = -1\n",
    "        \n",
    "        \n",
    "    def visualize_grid(self):\n",
    "        # Create a custom color map\n",
    "        cmap = plt.cm.colors.ListedColormap(['red', 'grey', 'white', 'yellow', 'green'])\n",
    "        bounds = [-2.5, -1.5, -0.5, 1.5, 2.5, 3.5]\n",
    "        norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(self.grid, cmap=cmap, norm=norm)\n",
    "\n",
    "        # # Mark the start and goal positions with text\n",
    "        # ax.text(self.goal[1], self.goal[0], 'G', va='center', ha='center', color='black')\n",
    "\n",
    "        # Adjust grid visibility\n",
    "        ax.grid(which='major', color='k', linestyle='-', linewidth=2)\n",
    "        ax.set_xticks(np.arange(-.5, self.width, 1), minor=False)\n",
    "        ax.set_yticks(np.arange(-.5, self.height, 1), minor=False)\n",
    "        ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def show_q_table(self):\n",
    "        action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "\n",
    "        # Calculate the maximum width needed for the values\n",
    "        max_value_width = max(len(\"{:.2f}\".format(value)) for value in self.Q_table.flatten())\n",
    "        max_label_width = max(max_value_width, 7)  # Comparing with 'Action' word length\n",
    "\n",
    "        # Prepare the header with uniform spacing\n",
    "        header = f\"{'State':<{max_label_width}} | \" + \" | \".join([f\"{name:^{max_label_width}}\" for name in action_names])\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "        # Iterate through each state in the Q-table\n",
    "        for state in range(self.num_states):\n",
    "            position = self.get_position(state)\n",
    "            state_label = f\"({position[0]}, {position[1]})\"\n",
    "\n",
    "            # Prepare the Q-values with uniform formatting\n",
    "            q_values_str = \" | \".join([f\"{q_value:^{max_label_width}.2f}\" for q_value in self.Q_table[state]])\n",
    "\n",
    "            # Print the state and its Q-values with consistent spacing\n",
    "            print(f\"{state_label:<{max_label_width}} | {q_values_str}\")\n",
    "\n",
    "    def get_state(self, position):\n",
    "        \"\"\"Converts a grid position to a state integer.\"\"\"\n",
    "        return position[0] * self.width + position[1]\n",
    "\n",
    "    def get_position(self, state):\n",
    "        \"\"\"Converts a state integer back to grid position.\"\"\"\n",
    "        return (state // self.width, state % self.width)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"Returns the next state, reward, and done flag given a state and action.\"\"\"\n",
    "        row, col = self.get_position(state)\n",
    "        if action == 0 and row > 0:  # Up\n",
    "            row -= 1\n",
    "        elif action == 1 and row < self.height - 1:  # Down\n",
    "            row += 1\n",
    "        elif action == 2 and col > 0:  # Left\n",
    "            col -= 1\n",
    "        elif action == 3 and col < self.width - 1:  # Right\n",
    "            col += 1\n",
    "\n",
    "        next_state = self.get_state((row, col))\n",
    "        done = (row, col) == self.goal\n",
    "        reward = self.goal_reward if done else self.hazard_reward if (row, col) in self.hazard else self.obstacle_reward if (row, col) in self.obstacles else self.free_space_reward\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "    def q_learning(self, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        for episode in range(episodes):\n",
    "            state = self.get_state(self.start)\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = np.random.choice(self.actions)  # Explore\n",
    "                else:\n",
    "                    action = np.argmax(self.Q_table[state])  # Exploit\n",
    "\n",
    "                next_state, reward, done = self.step(state, action)\n",
    "\n",
    "                # Update Q-table using the Q-learning formula\n",
    "                self.Q_table[state, action] += alpha * (reward + gamma * np.max(self.Q_table[next_state]) - self.Q_table[state, action])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "    def create_trajectory(self):\n",
    "        # Reset environment to start\n",
    "        state = self.get_state(self.start)\n",
    "        trajectory = [self.start]\n",
    "        actions_taken = []\n",
    "        total_reward = 0  # Initialize total reward\n",
    "\n",
    "        # Follow the policy derived from Q-table\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "            next_state, reward, done = self.step(state, action)\n",
    "            actions_taken.append(action)\n",
    "            trajectory.append(self.get_position(next_state))\n",
    "            total_reward += reward  # Accumulate the reward\n",
    "\n",
    "            # Terminate if total reward drops below -15\n",
    "            if total_reward < -15:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return trajectory, actions_taken, total_reward\n",
    "\n",
    "    def visualize_grid_with_trajectory(self, trajectory, actions):\n",
    "        fig, ax = plt.subplots()\n",
    "        # Create a custom color map\n",
    "        cmap = plt.cm.colors.ListedColormap(['red', 'grey', 'white', 'yellow', 'green'])\n",
    "        bounds = [-2.5, -1.5, -0.5, 1.5, 2.5, 3.5]\n",
    "        norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "        # Display the grid\n",
    "        ax.imshow(self.grid, cmap=cmap, norm=norm)\n",
    "    \n",
    "        # Calculate arrow properties\n",
    "        arrow_length = 0.95  # Full length from center to center\n",
    "        arrow_width = 0.03\n",
    "        head_width = 0.15\n",
    "        head_length = 0.15\n",
    "    \n",
    "        # Draw arrows for actions taken in the trajectory\n",
    "        for i in range(len(trajectory) - 1):\n",
    "            start_cell = trajectory[i]\n",
    "            action = actions[i]\n",
    "    \n",
    "            # Starting point (center of the cell)\n",
    "            start_x = start_cell[1] \n",
    "            start_y = start_cell[0] \n",
    "    \n",
    "            # Determine the direction of the arrow based on the action\n",
    "            if action == 0:  # Up\n",
    "                dx, dy = 0, -arrow_length\n",
    "            elif action == 1:  # Down\n",
    "                dx, dy = 0, arrow_length\n",
    "            elif action == 2:  # Left\n",
    "                dx, dy = -arrow_length, 0\n",
    "            elif action == 3:  # Right\n",
    "                dx, dy = arrow_length, 0\n",
    "            else:\n",
    "                continue  # Skip if the action is not recognized\n",
    "    \n",
    "            # Draw the arrow from center to center\n",
    "            ax.arrow(start_x, start_y, dx, dy, head_width=head_width, head_length=head_length, fc='black', ec='black', length_includes_head=True, shape='full', width=arrow_width)\n",
    "    \n",
    "        # Customize plot to match the grid's appearance\n",
    "        ax.grid(which='major', color='k', linestyle='-', linewidth=2)\n",
    "        ax.set_xticks(np.arange(-.5, self.width, 1), minor=False)\n",
    "        ax.set_yticks(np.arange(-.5, self.height, 1), minor=False)\n",
    "        ax.tick_params(axis=u'both', which=u'both', length=0)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and visualize the grid\n",
    "grid = Gridworld(width=5, height=4)\n",
    "grid.visualize_grid()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:43:06.288751Z",
     "start_time": "2024-02-23T23:43:06.123805Z"
    }
   },
   "id": "2fe4c1d67ede702a",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grid.q_learning(episodes=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:43:06.682942Z",
     "start_time": "2024-02-23T23:43:06.641944Z"
    }
   },
   "id": "faa56b0e4f21d22a",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 4\n",
      "Actions: [3, 3, 1, 3, 1, 3, 1]\n",
      "Trajectory: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4)]\n",
      "\n",
      "Q-Table:\n",
      "\n",
      "State   |   Up    |  Down   |  Left   |  Right \n",
      "-----------------------------------------------\n",
      "(0, 0)  |  -3.32  |  -3.29  |  -3.27  |  1.80  \n",
      "(0, 1)  |  -1.60  |  -2.72  |  -2.74  |  3.54  \n",
      "(0, 2)  |  -1.51  |  5.14   |  -1.80  |  -4.16 \n",
      "(0, 3)  |  -1.91  |  1.59   |  -0.40  |  -0.30 \n",
      "(0, 4)  |  -0.30  |  0.30   |  -1.00  |  -0.20 \n",
      "(1, 0)  |  -2.97  |  -2.97  |  -2.95  |  -3.10 \n",
      "(1, 1)  |  -1.04  |  -0.83  |  -0.98  |  0.79  \n",
      "(1, 2)  |  -0.51  |  -1.74  |  -2.55  |  6.54  \n",
      "(1, 3)  |  -3.97  |  7.76   |  0.06   |  -0.25 \n",
      "(1, 4)  |  -0.27  |  4.83   |  -0.21  |  -0.20 \n",
      "(2, 0)  |  -2.70  |  -2.64  |  -2.65  |  -2.69 \n",
      "(2, 1)  |  -2.52  |  -2.45  |  -2.45  |  -2.39 \n",
      "(2, 2)  |  -0.23  |  -0.12  |  -0.34  |  3.11  \n",
      "(2, 3)  |  -0.21  |  -0.19  |  -1.38  |  8.89  \n",
      "(2, 4)  |  0.67   |  10.00  |  2.24   |  0.00  \n",
      "(3, 0)  |  -2.55  |  -2.47  |  -2.47  |  -2.73 \n",
      "(3, 1)  |  -0.41  |  -1.39  |  -0.48  |  -0.29 \n",
      "(3, 2)  |  -0.50  |  -0.20  |  -0.99  |  1.96  \n",
      "(3, 3)  |  -0.10  |  -0.10  |  -0.10  |  6.51  \n",
      "(3, 4)  |  0.00   |  0.00   |  0.00   |  0.00  \n"
     ]
    }
   ],
   "source": [
    "trajectory, actions, reward = grid.create_trajectory()\n",
    "grid.visualize_grid_with_trajectory(trajectory, actions)\n",
    "\n",
    "print(f\"Total Reward: {reward}\\n\"\n",
    "      f\"Actions: {actions}\\n\"\n",
    "      f\"Trajectory: {trajectory}\\n\\n\"\n",
    "      f\"Q-Table:\\n\")\n",
    "\n",
    "grid.show_q_table()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T23:43:07.458824Z",
     "start_time": "2024-02-23T23:43:07.311803Z"
    }
   },
   "id": "8a702bc170b274b3",
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
